# Guide de Reproduction - CV Classifier

Ce guide explique comment reproduire la solution compl√®te du classificateur de CV, de l'installation jusqu'√† l'interface web.

---

## Table des Mati√®res

1. [Structure du Projet](#1-structure-du-projet)
2. [Classification des Fichiers](#2-classification-des-fichiers)
3. [Pr√©requis](#3-pr√©requis)
4. [Installation](#4-installation)
5. [Ordre d'Ex√©cution](#5-ordre-dex√©cution)
6. [Description des Fichiers Essentiels](#6-description-des-fichiers-essentiels)
7. [Fichiers d'Exploration (Notebooks)](#7-fichiers-dexploration-notebooks)
8. [Fichiers Legacy (Non Utilis√©s)](#8-fichiers-legacy-non-utilis√©s)
9. [Lancement de l'API](#9-lancement-de-lapi)
10. [Troubleshooting](#10-troubleshooting)

---

## 1. Structure du Projet

```
Projet_NLPfinal/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ resume_dataset.csv       # Dataset brut (962 CVs)
‚îÇ   ‚îú‚îÄ‚îÄ processed/                    # Donn√©es nettoy√©es (g√©n√©r√©)
‚îÇ   ‚îî‚îÄ‚îÄ splits/                       # Indices train/test (g√©n√©r√©)
‚îÇ       ‚îú‚îÄ‚îÄ train_indices.json
‚îÇ       ‚îú‚îÄ‚îÄ test_indices.json
‚îÇ       ‚îî‚îÄ‚îÄ split_metadata.json
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_cleaner.py          # Nettoyage de texte
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py              # Module exports
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_splitter.py         # Split des donn√©es brutes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformers.py          # Transformers sklearn
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_builder.py      # Construction du pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py               # Entra√Ænement + CV
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py             # √âvaluation finale
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ db_manager.py            # Gestion historique
‚îÇ   ‚îú‚îÄ‚îÄ pdf_processing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pdf_extractor.py         # Extraction PDF
‚îÇ   ‚îî‚îÄ‚îÄ skills_extraction/
‚îÇ       ‚îî‚îÄ‚îÄ skills_detector.py       # D√©tection comp√©tences
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ train_pipeline.py            # Script principal d'entra√Ænement
‚îÇ
‚îú‚îÄ‚îÄ models/                           # Mod√®les entra√Æn√©s (g√©n√©r√©)
‚îÇ   ‚îú‚îÄ‚îÄ cv_classifier_pipeline.pkl   # Pipeline complet
‚îÇ   ‚îú‚îÄ‚îÄ label_encoder.pkl            # Encodeur de labels
‚îÇ   ‚îú‚îÄ‚îÄ cv_results.json              # M√©triques CV
‚îÇ   ‚îú‚îÄ‚îÄ test_evaluation.json         # M√©triques test
‚îÇ   ‚îî‚îÄ‚îÄ training_metadata.json       # M√©tadonn√©es
‚îÇ
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ config.py                    # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ frontend/
‚îÇ       ‚îî‚îÄ‚îÄ index.html               # Interface web
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_api.py              # Tests API
‚îÇ   ‚îî‚îÄ‚îÄ unit/
‚îÇ       ‚îî‚îÄ‚îÄ test_text_cleaner.py     # Tests unitaires
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                  # D√©pendances Python
‚îî‚îÄ‚îÄ GUIDE_REPRODUCTION.md            # Ce fichier
```

---

## 2. Classification des Fichiers

### L√©gende

| Cat√©gorie | Symbole | Description |
|-----------|---------|-------------|
| **ESSENTIEL** | ‚úÖ | N√©cessaire pour la production |
| **EXPLORATION** | üî¨ | Notebooks pour exploration/d√©veloppement |
| **LEGACY** | ‚ö†Ô∏è | Ancien code, non utilis√© |
| **G√âN√âR√â** | üì¶ | G√©n√©r√© automatiquement |

---

### Vue Compl√®te des Fichiers

```
Projet_NLPfinal/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ resume_dataset.csv          # Dataset source (REQUIS)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ processed/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üì¶ resume_cleaned.csv          # G√©n√©r√© par notebooks (legacy)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üì¶ resume_cleaned_compact.csv  # G√©n√©r√© (legacy)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üì¶ resume_with_stats.csv       # G√©n√©r√© (legacy)
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ splits/
‚îÇ       ‚îú‚îÄ‚îÄ üì¶ train_indices.json          # G√©n√©r√© par train_pipeline.py
‚îÇ       ‚îú‚îÄ‚îÄ üì¶ test_indices.json           # G√©n√©r√© par train_pipeline.py
‚îÇ       ‚îî‚îÄ‚îÄ üì¶ split_metadata.json         # G√©n√©r√© par train_pipeline.py
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ preprocessing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ text_cleaner.py             # Nettoyage texte (UTILIS√â)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ training/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ __init__.py                 # Module exports
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ data_splitter.py            # Split anti-leakage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ transformers.py             # Wrapper sklearn
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ pipeline_builder.py         # Construction pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ trainer.py                  # Entra√Ænement + CV
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ evaluator.py                # √âvaluation finale
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ database/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ db_manager.py               # Historique (optionnel)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pdf_processing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ pdf_extractor.py            # Extraction PDF (optionnel)
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ skills_extraction/
‚îÇ       ‚îî‚îÄ‚îÄ ‚úÖ skills_detector.py          # D√©tection skills (optionnel)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ train_pipeline.py               # ‚≠ê SCRIPT PRINCIPAL
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/
‚îÇ       ‚îú‚îÄ‚îÄ ‚ö†Ô∏è check_models.py             # Legacy
‚îÇ       ‚îú‚îÄ‚îÄ ‚ö†Ô∏è reprocess_full_dataset.py   # Legacy
‚îÇ       ‚îî‚îÄ‚îÄ ‚ö†Ô∏è train_optimized.py          # Legacy
‚îÇ
‚îú‚îÄ‚îÄ üìÅ notebooks/                          # üî¨ EXPLORATION UNIQUEMENT
‚îÇ   ‚îú‚îÄ‚îÄ üî¨ 01_EDA.ipynb                    # Analyse exploratoire
‚îÇ   ‚îú‚îÄ‚îÄ üî¨ 02_preprocessing.ipynb          # Tests preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ üî¨ 03_feature_extraction.ipynb     # Tests TF-IDF
‚îÇ   ‚îú‚îÄ‚îÄ üî¨ 04_modeling.ipynb               # Tests mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ üî¨ 05_evaluation.ipynb             # Tests √©valuation
‚îÇ   ‚îî‚îÄ‚îÄ üî¨ 06_API_testing.ipynb            # Tests API
‚îÇ
‚îú‚îÄ‚îÄ üìÅ models/                             # üì¶ G√âN√âR√â
‚îÇ   ‚îú‚îÄ‚îÄ üì¶ cv_classifier_pipeline.pkl      # Pipeline complet
‚îÇ   ‚îú‚îÄ‚îÄ üì¶ label_encoder.pkl               # Encodeur labels
‚îÇ   ‚îú‚îÄ‚îÄ üì¶ cv_results.json                 # M√©triques CV
‚îÇ   ‚îú‚îÄ‚îÄ üì¶ test_evaluation.json            # M√©triques test
‚îÇ   ‚îú‚îÄ‚îÄ üì¶ training_metadata.json          # M√©tadonn√©es
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è best_model.pkl                  # Legacy (ancien mod√®le)
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è tfidf_vectorizer.pkl            # Legacy (ancien vectorizer)
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è Random_Forest_model.pkl         # Legacy
‚îÇ   ‚îî‚îÄ‚îÄ ‚ö†Ô∏è Gradient_Boosting_model.pkl     # Legacy
‚îÇ
‚îú‚îÄ‚îÄ üìÅ api/
‚îÇ   ‚îú‚îÄ‚îÄ ‚úÖ main.py                         # ‚≠ê API PRINCIPALE
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è config.py                       # Legacy (non utilis√©)
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è diagnostic_api.py               # Legacy
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è enhanced_endpoints.py           # Legacy
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è models.py                       # Legacy
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è predict_service.py              # Legacy
‚îÇ   ‚îú‚îÄ‚îÄ ‚ö†Ô∏è test_api.py                     # Legacy (remplac√© par tests/)
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ frontend/
‚îÇ       ‚îú‚îÄ‚îÄ ‚úÖ index.html                  # Interface principale
‚îÇ       ‚îî‚îÄ‚îÄ ‚ö†Ô∏è cv_classifier_final.html    # Legacy
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tests/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ integration/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ‚úÖ test_api.py                 # Tests API
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ unit/
‚îÇ       ‚îî‚îÄ‚îÄ ‚úÖ test_text_cleaner.py        # Tests unitaires
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/
‚îÇ   ‚îú‚îÄ‚îÄ üî¨ Architecture_Projet.html        # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ üî¨ mlflow_guide_complete.html      # Guide MLflow
‚îÇ
‚îú‚îÄ‚îÄ ‚úÖ requirements.txt                    # D√©pendances
‚îú‚îÄ‚îÄ ‚úÖ GUIDE_REPRODUCTION.md               # Ce fichier
‚îú‚îÄ‚îÄ ‚úÖ README.md                           # Pr√©sentation projet
‚îî‚îÄ‚îÄ ‚úÖ pytest.ini                          # Config tests
```

---

### R√©sum√© par Cat√©gorie

| Cat√©gorie | Nombre | Action |
|-----------|--------|--------|
| ‚úÖ Essentiel | 18 fichiers | Garder |
| üî¨ Exploration | 8 fichiers | Garder pour r√©f√©rence |
| ‚ö†Ô∏è Legacy | 15+ fichiers | Peuvent √™tre supprim√©s |
| üì¶ G√©n√©r√© | 10+ fichiers | R√©g√©n√©r√©s automatiquement |

---

## 3. Pr√©requis

### Logiciels requis

| Logiciel | Version minimale | V√©rification |
|----------|------------------|--------------|
| Python | 3.10+ | `python --version` |
| pip | 21.0+ | `pip --version` |
| Git | 2.0+ | `git --version` |

### Dataset

Le fichier `data/raw/resume_dataset.csv` doit contenir:
- Colonne `Resume`: Texte brut du CV
- Colonne `Category`: Cat√©gorie professionnelle (25 classes)

---

## 4. Installation

### √âtape 1: Cloner le projet

```bash
git clone https://github.com/Tchapda3002/nlp-project.git
cd Projet_NLPfinal
```

### √âtape 2: Cr√©er un environnement virtuel (recommand√©)

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

### √âtape 3: Installer les d√©pendances

```bash
pip install -r requirements.txt
```

**D√©pendances principales:**
```
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
nltk>=3.8.0
fastapi>=0.100.0
uvicorn>=0.23.0
joblib>=1.3.0
pdfplumber>=0.10.0      # Optionnel: extraction PDF
pytesseract>=0.3.10     # Optionnel: OCR
```

### √âtape 4: T√©l√©charger les ressources NLTK

```bash
python -c "
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('omw-1.4')
"
```

---

## 5. Ordre d'Ex√©cution

### Vue d'ensemble

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 1: DONN√âES                                               ‚îÇ
‚îÇ  data/raw/resume_dataset.csv (manuel)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 2: ENTRA√éNEMENT                                          ‚îÇ
‚îÇ  python scripts/train_pipeline.py                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  Ex√©cute automatiquement:                                       ‚îÇ
‚îÇ  1. src/training/data_splitter.py    ‚Üí Split train/test        ‚îÇ
‚îÇ  2. src/training/transformers.py     ‚Üí Nettoyage texte         ‚îÇ
‚îÇ  3. src/training/pipeline_builder.py ‚Üí Construction pipeline   ‚îÇ
‚îÇ  4. src/training/trainer.py          ‚Üí Cross-validation        ‚îÇ
‚îÇ  5. src/training/evaluator.py        ‚Üí √âvaluation finale       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 3: API                                                   ‚îÇ
‚îÇ  python -m uvicorn api.main:app --reload                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PHASE 4: INTERFACE                                             ‚îÇ
‚îÇ  http://localhost:8000/app                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Commandes √† ex√©cuter dans l'ordre

```bash
# 1. S'assurer que le dataset est pr√©sent
ls data/raw/resume_dataset.csv

# 2. Entra√Æner le mod√®le (g√©n√®re tout automatiquement)
python scripts/train_pipeline.py

# 3. V√©rifier que les mod√®les sont g√©n√©r√©s
ls models/

# 4. Lancer les tests (optionnel mais recommand√©)
python -m pytest tests/ -v

# 5. D√©marrer l'API
python -m uvicorn api.main:app --host 0.0.0.0 --port 8000

# 6. Acc√©der √† l'interface
# Ouvrir: http://localhost:8000/app
```

---

## 6. Description des Fichiers Essentiels

### Phase 1: Donn√©es

| Fichier | R√¥le | Quand l'utiliser |
|---------|------|------------------|
| `data/raw/resume_dataset.csv` | Dataset source avec 962 CVs | Doit exister avant l'entra√Ænement |

---

### Phase 2: Entra√Ænement

#### `scripts/train_pipeline.py` ‚≠ê POINT D'ENTR√âE PRINCIPAL

```bash
python scripts/train_pipeline.py [OPTIONS]
```

**Options:**
| Option | D√©faut | Description |
|--------|--------|-------------|
| `--classifier` | random_forest | Algorithme (random_forest, gradient_boosting, logistic_regression, naive_bayes, svm) |
| `--test-size` | 0.2 | Proportion du test set |
| `--n-folds` | 5 | Nombre de folds pour la CV |
| `--skip-cv` | False | Ignorer la cross-validation |
| `--force-new-split` | False | Forcer un nouveau split |

**Ce qu'il fait:**
1. Charge `data/raw/resume_dataset.csv`
2. Split 80/20 sur donn√©es BRUTES (anti data-leakage)
3. Sauvegarde les indices dans `data/splits/`
4. Ex√©cute une cross-validation 5-fold
5. Entra√Æne le mod√®le final
6. √âvalue sur le test set
7. Sauvegarde tout dans `models/`

---

#### `src/training/data_splitter.py`

**R√¥le:** S√©parer les donn√©es AVANT tout preprocessing

**Pourquoi c'est important:**
- √âvite la fuite de donn√©es (data leakage)
- Le test set ne doit JAMAIS influencer l'entra√Ænement
- Sauvegarde les indices pour reproductibilit√©

**Classe principale:**
```python
class DataSplitter:
    def split_and_save(df, target_column, output_dir)
    def load_split(df, split_dir)
    def split_exists(split_dir)
```

---

#### `src/training/transformers.py`

**R√¥le:** Wrapper sklearn autour de TextCleaner

**Pourquoi c'est important:**
- Permet d'int√©grer le nettoyage dans un Pipeline sklearn
- Le nettoyage est appliqu√© APR√àS le split
- Stateless: fit() ne fait rien, transform() nettoie

**Classe principale:**
```python
class TextCleanerTransformer(BaseEstimator, TransformerMixin):
    def fit(X, y=None)      # Ne fait rien (stateless)
    def transform(X)         # Nettoie le texte
```

---

#### `src/training/pipeline_builder.py`

**R√¥le:** Construire le pipeline sklearn complet

**Structure du pipeline:**
```
TextCleanerTransformer ‚Üí TfidfVectorizer ‚Üí Classifier
```

**Pourquoi c'est important:**
- Encapsule TOUTES les transformations
- Garantit que TF-IDF est fit UNIQUEMENT sur train
- Facilite la pr√©diction (une seule ligne)

**Classe principale:**
```python
class CVClassifierPipelineBuilder:
    def build()              # Retourne un Pipeline sklearn
    def get_param_grid()     # Pour GridSearchCV
```

---

#### `src/training/trainer.py`

**R√¥le:** Orchestrer l'entra√Ænement et la cross-validation

**Pourquoi c'est important:**
- Cross-validation sur train UNIQUEMENT
- Mesure la vraie performance de g√©n√©ralisation
- Sauvegarde les m√©triques pour tra√ßabilit√©

**Classe principale:**
```python
class CVClassifierTrainer:
    def cross_validate(X_train, y_train)  # CV 5-fold
    def train(X_train, y_train)           # Entra√Ænement final
    def save(output_dir)                  # Sauvegarde
```

---

#### `src/training/evaluator.py`

**R√¥le:** √âvaluation finale sur le test set

**Pourquoi c'est important:**
- Appel√© UNE SEULE FOIS √† la fin
- Donne la vraie performance sur donn√©es jamais vues
- Compare avec les r√©sultats de CV

**Classe principale:**
```python
class PipelineEvaluator:
    def evaluate(X_test, y_test)       # M√©triques compl√®tes
    def compare_with_cv(cv_results)    # D√©tection overfitting
    def save_report(output_dir)        # Rapport JSON + TXT
```

---

#### `src/preprocessing/text_cleaner.py`

**R√¥le:** Nettoyage du texte des CVs

**Transformations:**
1. Mise en minuscules
2. Suppression URLs, emails, t√©l√©phones
3. Suppression ponctuation
4. Tokenisation
5. Suppression stopwords
6. Lemmatisation

**Classe principale:**
```python
class TextCleaner:
    def clean_text(text)           # Nettoie un texte
    def clean_dataframe(df, col)   # Nettoie une colonne
```

---

### Phase 3: API

#### `api/main.py` ‚≠ê API PRINCIPALE

**R√¥le:** Exposer le mod√®le via REST API

**Endpoints principaux:**

| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/` | GET | Informations API |
| `/health` | GET | √âtat de sant√© |
| `/predict` | POST | Classifier un CV |
| `/model-info` | GET | M√©triques du mod√®le |
| `/categories` | GET | Liste des cat√©gories |
| `/upload-cv` | POST | Upload PDF + classification |
| `/analyze-skills` | POST | Extraction comp√©tences |
| `/app` | GET | Interface web |

**Chargement automatique:**
```python
# Charge dans cet ordre:
1. cv_classifier_pipeline.pkl   # Pipeline complet
2. label_encoder.pkl            # D√©codage labels
3. cv_results.json              # M√©triques CV
4. test_evaluation.json         # M√©triques test
5. training_metadata.json       # M√©tadonn√©es
```

---

#### `api/frontend/index.html`

**R√¥le:** Interface web interactive

**Fonctionnalit√©s:**
- Upload de CV (texte ou PDF)
- Options d'analyse (skills, exp√©rience, recommandations)
- Affichage des probabilit√©s top 5
- Historique des classifications

---

### Phase 4: Tests

#### `tests/integration/test_api.py`

**R√¥le:** Tester les endpoints API

```bash
python -m pytest tests/integration/ -v
```

#### `tests/unit/test_text_cleaner.py`

**R√¥le:** Tester le nettoyage de texte

```bash
python -m pytest tests/unit/ -v
```

---

## 7. Fichiers d'Exploration (Notebooks)

Les notebooks sont utilis√©s pour l'**exploration et le d√©veloppement**, mais ne sont **PAS n√©cessaires** pour la production.

### Quand utiliser les notebooks ?

| Notebook | Utilit√© | Quand l'ex√©cuter |
|----------|---------|------------------|
| `01_EDA.ipynb` | Analyse exploratoire des donn√©es | Pour comprendre le dataset |
| `02_preprocessing.ipynb` | Tester le nettoyage de texte | Pour ajuster les param√®tres de nettoyage |
| `03_feature_extraction.ipynb` | Tester TF-IDF | Pour optimiser les hyperparam√®tres |
| `04_modeling.ipynb` | Comparer diff√©rents mod√®les | Pour choisir le meilleur algorithme |
| `05_evaluation.ipynb` | Analyser les erreurs | Pour comprendre les faiblesses du mod√®le |
| `06_API_testing.ipynb` | Tester l'API manuellement | Pour debug |

### Ordre d'ex√©cution des notebooks (si n√©cessaire)

```
01_EDA.ipynb
     ‚îÇ
     ‚ñº
02_preprocessing.ipynb
     ‚îÇ
     ‚ñº
03_feature_extraction.ipynb
     ‚îÇ
     ‚ñº
04_modeling.ipynb
     ‚îÇ
     ‚ñº
05_evaluation.ipynb
     ‚îÇ
     ‚ñº
06_API_testing.ipynb (apr√®s avoir lanc√© l'API)
```

### Important

> ‚ö†Ô∏è Les notebooks peuvent contenir du code **avec fuite de donn√©es** (data leakage) car ils ont √©t√© cr√©√©s pendant la phase d'exploration.
>
> Pour l'entra√Ænement final, utilisez **TOUJOURS** `scripts/train_pipeline.py` qui impl√©mente les bonnes pratiques anti-leakage.

---

## 8. Fichiers Legacy (Non Utilis√©s)

Ces fichiers ont √©t√© cr√©√©s pendant le d√©veloppement mais ne sont **plus utilis√©s** dans le workflow actuel.

### Fichiers √† supprimer (optionnel)

```bash
# Scripts legacy
rm scripts/utils/check_models.py
rm scripts/utils/reprocess_full_dataset.py
rm scripts/utils/train_optimized.py

# API legacy
rm api/config.py
rm api/diagnostic_api.py
rm api/enhanced_endpoints.py
rm api/models.py
rm api/predict_service.py
rm api/test_api.py

# Frontend legacy
rm api/frontend/cv_classifier_final.html

# Anciens mod√®les (remplac√©s par le pipeline)
rm models/best_model.pkl
rm models/tfidf_vectorizer.pkl
rm models/Random_Forest_model.pkl
rm models/Gradient_Boosting_model.pkl

# Donn√©es process√©es (le pipeline les r√©g√©n√®re)
rm data/processed/resume_cleaned.csv
rm data/processed/resume_cleaned_compact.csv
rm data/processed/resume_with_stats.csv
```

### Pourquoi ces fichiers existent ?

| Fichier | Historique |
|---------|------------|
| `best_model.pkl` | Ancien mod√®le entra√Æn√© SANS split correct |
| `tfidf_vectorizer.pkl` | Vectorizer fit sur TOUTES les donn√©es (leakage) |
| `api/config.py` | Configuration non utilis√©e |
| `scripts/utils/*.py` | Anciens scripts remplac√©s par `train_pipeline.py` |

---

## 9. Lancement de l'API

### D√©veloppement

```bash
python -m uvicorn api.main:app --reload --host 0.0.0.0 --port 8000
```

### Production

```bash
python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Acc√®s

| URL | Description |
|-----|-------------|
| http://localhost:8000 | Racine API |
| http://localhost:8000/app | Interface web |
| http://localhost:8000/docs | Documentation Swagger |
| http://localhost:8000/redoc | Documentation ReDoc |

---

## 10. Troubleshooting

### Erreur: "Module not found"

```bash
# V√©rifier que vous √™tes dans le bon dossier
pwd  # Doit afficher .../Projet_NLPfinal

# R√©installer les d√©pendances
pip install -r requirements.txt
```

### Erreur: "Dataset not found"

```bash
# V√©rifier que le dataset existe
ls -la data/raw/resume_dataset.csv

# Si manquant, placer le fichier CSV dans data/raw/
```

### Erreur: "Pipeline not found"

```bash
# R√©entra√Æner le mod√®le
python scripts/train_pipeline.py
```

### Erreur: "Port already in use"

```bash
# Tuer le processus existant
pkill -f "uvicorn.*main:app"

# Ou utiliser un autre port
python -m uvicorn api.main:app --port 8001
```

### Erreur NLTK

```bash
python -c "
import nltk
nltk.download('all')
"
```

---

## R√©sum√©: Commandes Essentielles

```bash
# Installation compl√®te
pip install -r requirements.txt

# Entra√Ænement
python scripts/train_pipeline.py

# Tests
python -m pytest tests/ -v

# Lancement API
python -m uvicorn api.main:app --reload

# Acc√®s interface
open http://localhost:8000/app
```

---

## M√©triques Attendues

Apr√®s entra√Ænement, vous devriez obtenir:

| M√©trique | Cross-Validation | Test Set |
|----------|------------------|----------|
| Accuracy | ~99.2% | ~100% |
| F1 Macro | ~99.0% | ~100% |
| Precision | ~99.3% | ~100% |
| Recall | ~98.9% | ~100% |

Ces m√©triques sont sauvegard√©es dans `models/` et charg√©es dynamiquement par l'API.
